{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lab3_tools'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-592dc8e3047a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mlab3_tools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'../lab1/amrita'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'../lab2/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'lab3_tools'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from lab3_tools import *\n",
    "import sys\n",
    "sys.path.insert(0, '../lab1/amrita')\n",
    "sys.path.insert(1, '../lab2/')\n",
    "from lab1_proto import mfcc,mspec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the emitting states in the phoneHMMs models from Lab 2 as target\n",
    "classes for the deep neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "phoneHMMs = np.load('../lab2/lab2_models_all.npz',allow_pickle=True)['phoneHMMs'].item()\n",
    "phones = sorted(phoneHMMs.keys())\n",
    "nstates = {phone: phoneHMMs[phone]['means'].shape[0] for phone in phones}\n",
    "stateList = [ph + '_' + str(id) for ph in phones for id in range(nstates[ph])]\n",
    "# np.save('StateList',stateList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the numerical index of a particular state can be obtained by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = np.load('data/lab3_example.npz',allow_pickle=True)['example'].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['filename', 'samples', 'gender', 'speaker', 'digits', 'repetition', 'lmfcc', 'wordTrans', 'phoneTrans', 'utteranceHMM', 'stateTrans', 'obsloglik', 'viterbiLoglik', 'viterbiPath', 'viterbiStateTrans'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stateList=list(np.load('data/StateList.npy'))\n",
    "stateList.index('ay_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples verified: True\n",
      "MFCC verified: True\n"
     ]
    }
   ],
   "source": [
    "filename = 'data/tidigits/disc_4.1.1/tidigits/train/man/nw/z43a.wav'\n",
    "samples, samplingrate = loadAudio(filename)\n",
    "print('Samples verified:',np.array_equal(example['samples'],samples))\n",
    "lmfcc = mfcc(samples,)\n",
    "print('MFCC verified:',np.allclose(example['lmfcc'],lmfcc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wordTrans verified: True\n"
     ]
    }
   ],
   "source": [
    "wordTrans = list(path2info(filename)[2])\n",
    "\n",
    "print('wordTrans verified:',np.array_equal(example['wordTrans'],wordTrans))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words2phones(wordList, pronDict, addSilence=True, addShortPause=True):\n",
    "    \"\"\" word2phones: converts word level to phone level transcription adding silence\n",
    "\n",
    "    Args:\n",
    "       wordList: list of word symbols\n",
    "       pronDict: pronunciation dictionary. The keys correspond to words in wordList\n",
    "       addSilence: if True, add initial and final silence\n",
    "       addShortPause: if True, add short pause model \"sp\" at end of each word\n",
    "    Output:\n",
    "       list of phone symbols\n",
    "    \"\"\"\n",
    "    phoneSymbols=['sil']\n",
    "    for word in wordList:\n",
    "        phoneSymbols+=pronDict[word]\n",
    "        phoneSymbols.append('sp')\n",
    "    phoneSymbols.append('sil')\n",
    "    return phoneSymbols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phoneTrans verified: True\n"
     ]
    }
   ],
   "source": [
    "from prondict import prondict\n",
    "phoneTrans = words2phones(wordTrans, prondict)\n",
    "print('phoneTrans verified:',np.array_equal(example['phoneTrans'],phoneTrans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utteranceHMM verified :\n",
      "startprob verified : True\n",
      "transmat verified : True\n",
      "means verified : True\n",
      "covars verified : True\n"
     ]
    }
   ],
   "source": [
    "from lab2_proto import concatHMMs\n",
    "utteranceHMM = concatHMMs(phoneHMMs, phoneTrans)\n",
    "print('utteranceHMM verified :')\n",
    "print('startprob verified :',np.array_equal(utteranceHMM['startprob'],example['utteranceHMM']['startprob']))\n",
    "print('transmat verified :',np.array_equal(utteranceHMM['transmat'],example['utteranceHMM']['transmat']))\n",
    "print('means verified :',np.array_equal(utteranceHMM['means'],example['utteranceHMM']['means']))\n",
    "print('covars verified :',np.array_equal(utteranceHMM['covars'],example['utteranceHMM']['covars']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'sp',\n",
       " 'startprob': array([0.1216644, 0.8783357]),\n",
       " 'transmat': array([[0.8656793, 0.1343206],\n",
       "        [0.       , 1.       ]]),\n",
       " 'means': array([[  38.44177  , -129.2183   ,   54.57983  ,   15.29446  ,\n",
       "           35.78878  ,   -6.647676 ,  -11.7913   ,  -25.44149  ,\n",
       "          -20.31962  ,  -27.77863  ,   -5.970622 ,    0.9313969,\n",
       "           22.0168   ]]),\n",
       " 'covars': array([[ 1672.14 ,  2563.211,  2475.374,  3410.646,  4425.932,  5530.672,\n",
       "          7249.499,  9209.617, 10143.23 ,  9683.029,  8902.944,  8219.629,\n",
       "          7589.251]])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phoneHMMs['sp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stateTrans verified : True\n"
     ]
    }
   ],
   "source": [
    "stateTrans = [phone + '_' + str(stateid) for phone in phoneTrans for stateid in range(nstates[phone])]\n",
    "print('stateTrans verified :',np.array_equal(stateTrans,example['stateTrans']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lab2_tools import log_multivariate_normal_density_diag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obsloglik verified : True\n"
     ]
    }
   ],
   "source": [
    "obsloglik=log_multivariate_normal_density_diag(lmfcc,utteranceHMM['means'],utteranceHMM['covars'])\n",
    "print('obsloglik verified :',np.allclose(obsloglik,example['obsloglik']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(log_emlik, log_startprob, log_transmat, forceFinalState=True):\n",
    "    \"\"\"Viterbi path.\n",
    "    Args:\n",
    "        log_emlik: NxM array of emission log likelihoods, N frames, M states\n",
    "        log_startprob: log probability to start in state i\n",
    "        log_transmat: transition log probability from state i to j\n",
    "        forceFinalState: if True, start backtracking from the final state in\n",
    "                  the model, instead of the best state at the last time step\n",
    "    Output:\n",
    "        viterbi_loglik: log likelihood of the best path\n",
    "        viterbi_path: best path\n",
    "    \"\"\"\n",
    "    N, M = log_emlik.shape\n",
    "    B = np.zeros((N, M))\n",
    "    V = np.zeros((N, M))\n",
    "    V[0] = np.log(log_startprob) + log_emlik[0]\n",
    "\n",
    "    for t in range(1, N):\n",
    "        for j in range(M):\n",
    "            V[t][j] = np.max(V[t-1] + np.log(log_transmat[:,j])) + log_emlik[t][j]\n",
    "            B[t][j] = np.argmax(V[t-1] + np.log(log_transmat[:,j]))\n",
    "    \n",
    "    best_score = np.max(V[t])\n",
    "    \n",
    "    path = np.zeros(N)\n",
    "    path[t] = np.argmax(B[t, :])\n",
    "\n",
    "    for t in range(N-2, 0, -1):\n",
    "        path[t] = B[t+1,int(path[t+1])]\n",
    "    \n",
    "    return best_score,path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "viterbiPath verified : True\n"
     ]
    }
   ],
   "source": [
    "_,viterbiPath=viterbi(obsloglik,utteranceHMM['startprob'][:-1],utteranceHMM['transmat'][:-1,:-1])\n",
    "viterbiPath = list(map(int, viterbiPath))\n",
    "print('viterbiPath verified :',np.array_equal(viterbiPath,example['viterbiPath']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "viterbiStateTrans verified : True\n"
     ]
    }
   ],
   "source": [
    "stateTrans=np.asarray(stateTrans)\n",
    "viterbiStateTrans=stateTrans[viterbiPath]\n",
    "print('viterbiStateTrans verified :',np.array_equal(viterbiStateTrans,np.asarray(example['viterbiStateTrans'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0 0.01 ah_0\\n0.01 0.02 ah_1\\n0.02 0.03 ah_2\\n0.03 0.04 ao_0\\n0.04 0.05 ao_1\\n0.05 0.060000000000000005 ao_2\\n0.060000000000000005 0.07 ay_0\\n0.07 0.08 ay_1\\n0.08 0.09 ay_2\\n0.09 0.09999999999999999 eh_0\\n0.09999999999999999 0.10999999999999999 eh_1\\n0.10999999999999999 0.11999999999999998 eh_2\\n0.11999999999999998 0.12999999999999998 ey_0\\n0.12999999999999998 0.13999999999999999 ey_1\\n0.13999999999999999 0.15 ey_2\\n0.15 0.16 f_0\\n0.16 0.17 f_1\\n0.17 0.18000000000000002 f_2\\n0.18000000000000002 0.19000000000000003 ih_0\\n0.19000000000000003 0.20000000000000004 ih_1\\n0.20000000000000004 0.21000000000000005 ih_2\\n0.21000000000000005 0.22000000000000006 iy_0\\n0.22000000000000006 0.23000000000000007 iy_1\\n0.23000000000000007 0.24000000000000007 iy_2\\n0.24000000000000007 0.25000000000000006 k_0\\n0.25000000000000006 0.26000000000000006 k_1\\n0.26000000000000006 0.2700000000000001 k_2\\n0.2700000000000001 0.2800000000000001 n_0\\n0.2800000000000001 0.2900000000000001 n_1\\n0.2900000000000001 0.3000000000000001 n_2\\n0.3000000000000001 0.3100000000000001 ow_0\\n0.3100000000000001 0.3200000000000001 ow_1\\n0.3200000000000001 0.3300000000000001 ow_2\\n0.3300000000000001 0.34000000000000014 r_0\\n0.34000000000000014 0.35000000000000014 r_1\\n0.35000000000000014 0.36000000000000015 r_2\\n0.36000000000000015 0.37000000000000016 s_0\\n0.37000000000000016 0.38000000000000017 s_1\\n0.38000000000000017 0.3900000000000002 s_2\\n0.3900000000000002 0.4000000000000002 sil_0\\n0.4000000000000002 0.4100000000000002 sil_1\\n0.4100000000000002 0.4200000000000002 sil_2\\n0.4200000000000002 0.4300000000000002 sp_0\\n0.4300000000000002 0.4400000000000002 t_0\\n0.4400000000000002 0.45000000000000023 t_1\\n0.45000000000000023 0.46000000000000024 t_2\\n0.46000000000000024 0.47000000000000025 th_0\\n0.47000000000000025 0.48000000000000026 th_1\\n0.48000000000000026 0.49000000000000027 th_2\\n0.49000000000000027 0.5000000000000002 uw_0\\n0.5000000000000002 0.5100000000000002 uw_1\\n0.5100000000000002 0.5200000000000002 uw_2\\n0.5200000000000002 0.5300000000000002 v_0\\n0.5300000000000002 0.5400000000000003 v_1\\n0.5400000000000003 0.5500000000000003 v_2\\n0.5500000000000003 0.5600000000000003 w_0\\n0.5600000000000003 0.5700000000000003 w_1\\n0.5700000000000003 0.5800000000000003 w_2\\n0.5800000000000003 0.5900000000000003 z_0\\n0.5900000000000003 0.6000000000000003 z_1\\n0.6000000000000003 0.6100000000000003 z_2\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames2trans(stateList, outfilename='z43a.lab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lab1_proto import mspec\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "stateList=list(np.load('data/StateList.npy'))\n",
    "targets=[]\n",
    "for i in viterbiStateTrans:\n",
    "    # print(i)\n",
    "    targets.append(stateList.index(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 77/77 [00:20<00:00,  3.68it/s]\n",
      "100%|██████████| 77/77 [00:18<00:00,  4.08it/s]\n",
      "100%|██████████| 77/77 [00:27<00:00,  2.77it/s]\n",
      "100%|██████████| 77/77 [00:20<00:00,  3.82it/s]\n",
      "100%|██████████| 77/77 [00:21<00:00,  3.63it/s]\n",
      "100%|██████████| 77/77 [00:26<00:00,  2.87it/s]\n",
      "100%|██████████| 77/77 [00:23<00:00,  3.27it/s]\n",
      "100%|██████████| 77/77 [00:22<00:00,  3.38it/s]\n",
      "100%|██████████| 77/77 [00:22<00:00,  3.40it/s]\n",
      "100%|██████████| 77/77 [00:27<00:00,  2.79it/s]\n",
      "100%|██████████| 77/77 [00:27<00:00,  2.85it/s]\n",
      "100%|██████████| 77/77 [00:23<00:00,  3.22it/s]\n",
      "100%|██████████| 77/77 [00:26<00:00,  2.91it/s]\n",
      "100%|██████████| 77/77 [00:25<00:00,  3.00it/s]\n",
      "100%|██████████| 77/77 [00:27<00:00,  2.78it/s]\n",
      "100%|██████████| 77/77 [00:29<00:00,  2.63it/s]\n",
      "100%|██████████| 77/77 [00:26<00:00,  2.85it/s]\n",
      "100%|██████████| 77/77 [00:28<00:00,  2.74it/s]\n",
      "100%|██████████| 77/77 [00:23<00:00,  3.30it/s]\n",
      "100%|██████████| 77/77 [00:25<00:00,  2.99it/s]\n",
      "100%|██████████| 77/77 [00:23<00:00,  3.22it/s]\n",
      "100%|██████████| 77/77 [00:25<00:00,  3.04it/s]\n",
      "100%|██████████| 77/77 [00:24<00:00,  3.19it/s]\n",
      "100%|██████████| 77/77 [00:21<00:00,  3.51it/s]\n",
      "100%|██████████| 77/77 [00:24<00:00,  3.12it/s]\n",
      "100%|██████████| 77/77 [00:31<00:00,  2.48it/s]\n",
      "100%|██████████| 77/77 [00:25<00:00,  3.00it/s]\n",
      "100%|██████████| 77/77 [00:26<00:00,  2.90it/s]\n",
      "100%|██████████| 77/77 [00:25<00:00,  2.98it/s]\n",
      "100%|██████████| 77/77 [00:23<00:00,  3.30it/s]\n",
      "100%|██████████| 77/77 [00:25<00:00,  3.06it/s]\n",
      "100%|██████████| 77/77 [00:20<00:00,  3.74it/s]\n",
      "100%|██████████| 77/77 [00:23<00:00,  3.21it/s]\n",
      "100%|██████████| 77/77 [00:28<00:00,  2.70it/s]\n",
      "100%|██████████| 77/77 [00:27<00:00,  2.77it/s]\n",
      "100%|██████████| 77/77 [00:28<00:00,  2.70it/s]\n",
      "100%|██████████| 77/77 [00:29<00:00,  2.65it/s]\n",
      "100%|██████████| 77/77 [00:26<00:00,  2.87it/s]\n",
      "100%|██████████| 77/77 [00:23<00:00,  3.27it/s]\n",
      "100%|██████████| 77/77 [00:26<00:00,  2.85it/s]\n",
      "100%|██████████| 77/77 [00:26<00:00,  2.89it/s]\n",
      "100%|██████████| 77/77 [00:28<00:00,  2.69it/s]\n",
      "100%|██████████| 77/77 [00:25<00:00,  2.97it/s]\n",
      "100%|██████████| 77/77 [00:24<00:00,  3.17it/s]\n",
      "100%|██████████| 77/77 [00:25<00:00,  2.96it/s]\n",
      "100%|██████████| 77/77 [00:28<00:00,  2.69it/s]\n",
      "100%|██████████| 77/77 [00:25<00:00,  3.03it/s]\n",
      "100%|██████████| 77/77 [00:27<00:00,  2.80it/s]\n",
      "100%|██████████| 77/77 [00:30<00:00,  2.52it/s]\n",
      "100%|██████████| 77/77 [00:24<00:00,  3.20it/s]\n",
      "100%|██████████| 77/77 [00:25<00:00,  2.98it/s]\n",
      "100%|██████████| 77/77 [00:26<00:00,  2.92it/s]\n",
      "100%|██████████| 77/77 [00:28<00:00,  2.66it/s]\n",
      "100%|██████████| 77/77 [00:25<00:00,  3.00it/s]\n",
      "100%|██████████| 77/77 [00:25<00:00,  3.05it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 77/77 [00:25<00:00,  3.02it/s]\n",
      "100%|██████████| 76/76 [00:23<00:00,  3.22it/s]\n",
      "100%|██████████| 77/77 [00:26<00:00,  2.90it/s]\n",
      "100%|██████████| 77/77 [00:28<00:00,  2.67it/s]\n",
      "100%|██████████| 77/77 [00:25<00:00,  3.02it/s]\n",
      "100%|██████████| 77/77 [00:25<00:00,  3.01it/s]\n",
      "100%|██████████| 77/77 [00:31<00:00,  2.42it/s]\n",
      "100%|██████████| 77/77 [00:31<00:00,  2.48it/s]\n",
      "100%|██████████| 77/77 [00:32<00:00,  2.41it/s]\n",
      "100%|██████████| 77/77 [00:27<00:00,  2.81it/s]\n",
      "100%|██████████| 77/77 [00:27<00:00,  2.76it/s]\n",
      "100%|██████████| 77/77 [00:26<00:00,  2.93it/s]\n",
      "100%|██████████| 77/77 [00:24<00:00,  3.14it/s]\n",
      "100%|██████████| 77/77 [00:24<00:00,  3.10it/s]\n",
      "100%|██████████| 77/77 [00:25<00:00,  3.02it/s]\n",
      "100%|██████████| 77/77 [00:25<00:00,  3.06it/s]\n",
      "100%|██████████| 77/77 [00:26<00:00,  2.92it/s]\n",
      "100%|██████████| 77/77 [00:29<00:00,  2.64it/s]\n",
      "100%|██████████| 77/77 [00:23<00:00,  3.28it/s]\n",
      "100%|██████████| 77/77 [00:30<00:00,  2.52it/s]\n",
      "100%|██████████| 77/77 [00:29<00:00,  2.61it/s]\n",
      "100%|██████████| 77/77 [00:26<00:00,  2.90it/s]\n",
      "100%|██████████| 77/77 [00:29<00:00,  2.65it/s]\n",
      "100%|██████████| 77/77 [00:29<00:00,  2.62it/s]\n",
      "100%|██████████| 77/77 [00:26<00:00,  2.90it/s]\n",
      "100%|██████████| 77/77 [00:30<00:00,  2.56it/s]\n",
      "100%|██████████| 77/77 [00:25<00:00,  3.06it/s]\n",
      "100%|██████████| 77/77 [00:29<00:00,  2.65it/s]\n",
      "100%|██████████| 77/77 [00:23<00:00,  3.31it/s]\n",
      "100%|██████████| 77/77 [00:28<00:00,  2.72it/s]\n",
      "100%|██████████| 77/77 [00:33<00:00,  2.31it/s]\n",
      "100%|██████████| 77/77 [00:28<00:00,  2.68it/s]\n",
      "100%|██████████| 77/77 [00:27<00:00,  2.84it/s]\n",
      "100%|██████████| 77/77 [00:27<00:00,  2.80it/s]\n",
      "100%|██████████| 77/77 [00:27<00:00,  2.84it/s]\n",
      "100%|██████████| 77/77 [00:25<00:00,  2.98it/s]\n",
      "100%|██████████| 77/77 [00:29<00:00,  2.60it/s]\n",
      "100%|██████████| 77/77 [00:25<00:00,  3.01it/s]\n",
      "100%|██████████| 77/77 [00:24<00:00,  3.09it/s]\n",
      "100%|██████████| 77/77 [00:28<00:00,  2.73it/s]\n",
      "100%|██████████| 77/77 [00:35<00:00,  2.20it/s]\n",
      "100%|██████████| 77/77 [00:25<00:00,  2.97it/s]\n",
      "100%|██████████| 77/77 [00:34<00:00,  2.21it/s]\n",
      "100%|██████████| 77/77 [00:34<00:00,  2.23it/s]\n",
      "100%|██████████| 77/77 [00:22<00:00,  3.40it/s]\n",
      "100%|██████████| 77/77 [00:25<00:00,  3.02it/s]\n",
      "100%|██████████| 77/77 [00:24<00:00,  3.18it/s]\n",
      "100%|██████████| 77/77 [00:29<00:00,  2.63it/s]\n",
      "100%|██████████| 77/77 [00:22<00:00,  3.49it/s]\n",
      "100%|██████████| 77/77 [00:25<00:00,  3.02it/s]\n",
      "100%|██████████| 77/77 [00:30<00:00,  2.56it/s]\n",
      "100%|██████████| 77/77 [00:29<00:00,  2.59it/s]\n",
      "100%|██████████| 77/77 [00:27<00:00,  2.81it/s]\n",
      "100%|██████████| 77/77 [00:25<00:00,  3.04it/s]\n",
      "100%|██████████| 77/77 [00:25<00:00,  2.98it/s]\n",
      "100%|██████████| 77/77 [00:29<00:00,  2.63it/s]\n",
      "100%|██████████| 77/77 [00:26<00:00,  2.89it/s]\n"
     ]
    }
   ],
   "source": [
    "traindata = []\n",
    "for root, dirs, files in os.walk('data/tidigits/disc_4.1.1/tidigits/train'):\n",
    "    for file in tqdm(files):\n",
    "        if file.endswith('.wav'):\n",
    "            filename = os.path.join(root, file)\n",
    "            samples, samplingrate = loadAudio(filename)\n",
    "            lmfcc = mfcc(samples)\n",
    "            wordTrans = list(path2info(filename)[2])\n",
    "            phoneTrans = words2phones(wordTrans, prondict)\n",
    "            utteranceHMM = concatHMMs(phoneHMMs, phoneTrans)\n",
    "            obsloglik=log_multivariate_normal_density_diag(lmfcc,utteranceHMM['means'],utteranceHMM['covars'])\n",
    "            _,viterbiPath=viterbi(obsloglik,utteranceHMM['startprob'][:-1],utteranceHMM['transmat'][:-1,:-1])\n",
    "            viterbiPath = list(map(int, viterbiPath))\n",
    "            stateTrans = [phone + '_' + str(stateid) for phone in phoneTrans for stateid in range(nstates[phone])]\n",
    "            stateTrans=np.asarray(stateTrans)\n",
    "            viterbiStateTrans=stateTrans[viterbiPath]\n",
    "            targets=[]\n",
    "            for i in viterbiStateTrans:\n",
    "                targets.append(stateList.index(i))\n",
    "            mspec_=mspec(samples)\n",
    "            # stateList=stateTrans[viterbiStateTrans]\n",
    "            traindata.append({'filename': filename, 'lmfcc': lmfcc,'mspec': mspec_, 'targets': targets})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('data/train_data.npz', traindata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 77/77 [00:36<00:00,  2.11it/s]\n",
      "100%|██████████| 77/77 [00:24<00:00,  3.17it/s]\n",
      "100%|██████████| 76/76 [00:21<00:00,  3.46it/s]\n",
      "100%|██████████| 77/77 [00:20<00:00,  3.67it/s]\n",
      "100%|██████████| 77/77 [00:21<00:00,  3.62it/s]\n",
      "100%|██████████| 77/77 [00:22<00:00,  3.40it/s]\n",
      "100%|██████████| 77/77 [00:25<00:00,  2.99it/s]\n",
      "100%|██████████| 77/77 [00:31<00:00,  2.48it/s]\n",
      "100%|██████████| 77/77 [00:50<00:00,  1.53it/s]\n",
      "100%|██████████| 77/77 [01:02<00:00,  1.24it/s]\n",
      "100%|██████████| 77/77 [00:59<00:00,  1.29it/s]\n",
      "100%|██████████| 77/77 [00:47<00:00,  1.62it/s]\n",
      "100%|██████████| 77/77 [00:53<00:00,  1.44it/s]\n",
      "100%|██████████| 77/77 [00:55<00:00,  1.39it/s]\n",
      "100%|██████████| 77/77 [00:52<00:00,  1.46it/s]\n",
      "100%|██████████| 77/77 [01:00<00:00,  1.27it/s]\n",
      "100%|██████████| 77/77 [00:53<00:00,  1.43it/s]\n",
      "100%|██████████| 77/77 [00:38<00:00,  2.02it/s]\n",
      "100%|██████████| 77/77 [00:48<00:00,  1.60it/s]\n",
      "100%|██████████| 77/77 [00:45<00:00,  1.68it/s]\n",
      "100%|██████████| 77/77 [00:43<00:00,  1.77it/s]\n",
      "100%|██████████| 77/77 [00:31<00:00,  2.42it/s]\n",
      "100%|██████████| 77/77 [00:51<00:00,  1.50it/s]\n",
      "100%|██████████| 77/77 [01:00<00:00,  1.26it/s]\n",
      "100%|██████████| 77/77 [00:45<00:00,  1.70it/s]\n",
      "100%|██████████| 77/77 [00:56<00:00,  1.36it/s]\n",
      "100%|██████████| 77/77 [01:01<00:00,  1.26it/s]\n",
      "100%|██████████| 77/77 [00:58<00:00,  1.32it/s]\n",
      "100%|██████████| 77/77 [00:59<00:00,  1.30it/s]\n",
      "100%|██████████| 77/77 [01:19<00:00,  1.03s/it]\n",
      "100%|██████████| 77/77 [00:50<00:00,  1.52it/s]\n",
      "100%|██████████| 77/77 [00:51<00:00,  1.51it/s]\n",
      "100%|██████████| 77/77 [00:45<00:00,  1.69it/s]\n",
      "100%|██████████| 77/77 [00:54<00:00,  1.40it/s]\n",
      "100%|██████████| 77/77 [00:46<00:00,  1.66it/s]\n",
      "100%|██████████| 77/77 [00:51<00:00,  1.49it/s]\n",
      "100%|██████████| 77/77 [00:58<00:00,  1.31it/s]\n",
      "100%|██████████| 77/77 [01:00<00:00,  1.27it/s]\n",
      "100%|██████████| 77/77 [00:57<00:00,  1.35it/s]\n",
      "100%|██████████| 77/77 [00:44<00:00,  1.72it/s]\n",
      "100%|██████████| 77/77 [00:47<00:00,  1.63it/s]\n",
      "100%|██████████| 77/77 [00:50<00:00,  1.54it/s]\n",
      "100%|██████████| 77/77 [00:50<00:00,  1.52it/s]\n",
      "100%|██████████| 77/77 [00:51<00:00,  1.48it/s]\n",
      "100%|██████████| 77/77 [00:57<00:00,  1.33it/s]\n",
      "100%|██████████| 77/77 [00:51<00:00,  1.48it/s]\n",
      "100%|██████████| 77/77 [00:56<00:00,  1.37it/s]\n",
      "100%|██████████| 77/77 [01:14<00:00,  1.04it/s]\n",
      "100%|██████████| 77/77 [00:52<00:00,  1.48it/s]\n",
      "100%|██████████| 77/77 [00:55<00:00,  1.39it/s]\n",
      "100%|██████████| 77/77 [00:54<00:00,  1.41it/s]\n",
      "100%|██████████| 77/77 [00:53<00:00,  1.43it/s]\n",
      "100%|██████████| 77/77 [00:55<00:00,  1.40it/s]\n",
      "100%|██████████| 77/77 [00:53<00:00,  1.45it/s]\n",
      "100%|██████████| 77/77 [00:53<00:00,  1.44it/s]\n",
      "100%|██████████| 77/77 [00:55<00:00,  1.39it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 77/77 [01:02<00:00,  1.24it/s]\n",
      "100%|██████████| 77/77 [00:53<00:00,  1.43it/s]\n",
      "100%|██████████| 77/77 [01:01<00:00,  1.25it/s]\n",
      "100%|██████████| 77/77 [00:52<00:00,  1.46it/s]\n",
      "100%|██████████| 77/77 [00:33<00:00,  2.30it/s]\n",
      "100%|██████████| 77/77 [00:18<00:00,  4.10it/s]\n",
      "100%|██████████| 77/77 [00:24<00:00,  3.14it/s]\n",
      "100%|██████████| 77/77 [00:19<00:00,  4.00it/s]\n",
      "100%|██████████| 77/77 [00:18<00:00,  4.10it/s]\n",
      "100%|██████████| 77/77 [00:22<00:00,  3.46it/s]\n",
      "100%|██████████| 77/77 [00:19<00:00,  3.97it/s]\n",
      "100%|██████████| 77/77 [00:21<00:00,  3.55it/s]\n",
      "100%|██████████| 77/77 [00:21<00:00,  3.65it/s]\n",
      "100%|██████████| 77/77 [00:23<00:00,  3.26it/s]\n",
      "100%|██████████| 77/77 [00:23<00:00,  3.24it/s]\n",
      "100%|██████████| 77/77 [00:29<00:00,  2.59it/s]\n",
      "100%|██████████| 77/77 [00:26<00:00,  2.93it/s]\n",
      "100%|██████████| 77/77 [00:23<00:00,  3.21it/s]\n",
      "100%|██████████| 77/77 [00:21<00:00,  3.64it/s]\n",
      "100%|██████████| 77/77 [00:23<00:00,  3.32it/s]\n",
      "100%|██████████| 77/77 [00:29<00:00,  2.61it/s]\n",
      "100%|██████████| 77/77 [00:26<00:00,  2.95it/s]\n",
      "100%|██████████| 77/77 [00:25<00:00,  3.06it/s]\n",
      "100%|██████████| 77/77 [00:25<00:00,  3.01it/s]\n",
      "100%|██████████| 77/77 [00:21<00:00,  3.59it/s]\n",
      "100%|██████████| 77/77 [00:26<00:00,  2.96it/s]\n",
      "100%|██████████| 77/77 [00:23<00:00,  3.23it/s]\n",
      "100%|██████████| 77/77 [00:28<00:00,  2.71it/s]\n",
      "100%|██████████| 77/77 [00:23<00:00,  3.30it/s]\n",
      "100%|██████████| 77/77 [00:29<00:00,  2.63it/s]\n",
      "100%|██████████| 77/77 [00:21<00:00,  3.65it/s]\n",
      "100%|██████████| 77/77 [00:22<00:00,  3.35it/s]\n",
      "100%|██████████| 77/77 [00:25<00:00,  3.05it/s]\n",
      "100%|██████████| 77/77 [00:21<00:00,  3.57it/s]\n",
      "100%|██████████| 77/77 [00:27<00:00,  2.81it/s]\n",
      "100%|██████████| 77/77 [00:26<00:00,  2.92it/s]\n",
      "100%|██████████| 77/77 [00:29<00:00,  2.59it/s]\n",
      "100%|██████████| 77/77 [00:32<00:00,  2.34it/s]\n",
      "100%|██████████| 77/77 [00:37<00:00,  2.08it/s]\n",
      "100%|██████████| 77/77 [00:26<00:00,  2.88it/s]\n",
      "100%|██████████| 77/77 [00:26<00:00,  2.87it/s]\n",
      "100%|██████████| 77/77 [00:29<00:00,  2.58it/s]\n",
      "100%|██████████| 77/77 [00:37<00:00,  2.08it/s]\n",
      "100%|██████████| 77/77 [00:33<00:00,  2.32it/s]\n",
      "100%|██████████| 77/77 [00:34<00:00,  2.25it/s]\n",
      "100%|██████████| 77/77 [00:38<00:00,  2.02it/s]\n",
      "100%|██████████| 77/77 [00:41<00:00,  1.87it/s]\n",
      "100%|██████████| 77/77 [00:27<00:00,  2.82it/s]\n",
      "100%|██████████| 77/77 [00:31<00:00,  2.46it/s]\n",
      "100%|██████████| 77/77 [00:32<00:00,  2.35it/s]\n",
      "100%|██████████| 77/77 [00:32<00:00,  2.36it/s]\n",
      "100%|██████████| 77/77 [00:35<00:00,  2.14it/s]\n",
      "100%|██████████| 77/77 [00:32<00:00,  2.37it/s]\n",
      "100%|██████████| 77/77 [00:27<00:00,  2.82it/s]\n",
      "100%|██████████| 77/77 [00:25<00:00,  2.98it/s]\n",
      "100%|██████████| 77/77 [00:22<00:00,  3.45it/s]\n",
      "100%|██████████| 77/77 [00:21<00:00,  3.58it/s]\n"
     ]
    }
   ],
   "source": [
    "testdata = []\n",
    "for root, dirs, files in os.walk('data/tidigits/disc_4.2.1/tidigits/test'):\n",
    "    for file in tqdm(files):\n",
    "        if file.endswith('.wav'):\n",
    "            filename = os.path.join(root, file)\n",
    "            samples, samplingrate = loadAudio(filename)\n",
    "            lmfcc = mfcc(samples)\n",
    "            wordTrans = list(path2info(filename)[2])\n",
    "            phoneTrans = words2phones(wordTrans, prondict)\n",
    "            utteranceHMM = concatHMMs(phoneHMMs, phoneTrans)\n",
    "            obsloglik=log_multivariate_normal_density_diag(lmfcc,utteranceHMM['means'],utteranceHMM['covars'])\n",
    "            _,viterbiPath=viterbi(obsloglik,utteranceHMM['startprob'][:-1],utteranceHMM['transmat'][:-1,:-1])\n",
    "            viterbiPath = list(map(int, viterbiPath))\n",
    "            stateTrans = [phone + '_' + str(stateid) for phone in phoneTrans for stateid in range(nstates[phone])]\n",
    "            stateTrans=np.asarray(stateTrans)\n",
    "            viterbiStateTrans=stateTrans[viterbiPath]\n",
    "            targets=[]\n",
    "            for i in viterbiStateTrans:\n",
    "                targets.append(stateList.index(i))\n",
    "            mspec_=mspec(samples)\n",
    "            # stateList=stateTrans[viterbiStateTrans]\n",
    "            testdata.append({'filename': filename, 'lmfcc': lmfcc,'mspec': mspec_, 'targets': targets})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('data/testdata.npz', testdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('data/ters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-275a5c35a0ba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_val\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/traindata.npz'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mallow_pickle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'arr_0'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/testdata.npz'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mallow_pickle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'arr_0'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "train_val=np.load('data/traindata.npz',allow_pickle=True)['arr_0']\n",
    "test_data=np.load('data/testdata.npz',allow_pickle=True)['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8623,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = []\n",
    "train = []\n",
    "for x in train_val:\n",
    "    if x['filename'].endswith('b.wav') and 'woman' in x['filename']:\n",
    "        validation.append(x)\n",
    "    elif x['filename'].endswith('b.wav') and 'man' in x['filename'] :\n",
    "        validation.append(x)\n",
    "    else:\n",
    "        train.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 7373\n",
      "Validation size: 1250\n"
     ]
    }
   ],
   "source": [
    "print('Train size:',len(train))\n",
    "print('Validation size:',len(validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Split data into train and val set, stratify by gender & 4.5 Acoustic Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val=np.load('data/traindata.npz',allow_pickle=True)['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "dict_keys(['filename', 'lmfcc', 'mspec', 'targets'])"
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "train_val[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0\n[3, 2, 1, 0, 1, 2, 3]\n1\n[2, 1, 0, 1, 2, 3, 4]\n2\n[1, 0, 1, 2, 3, 4, 5]\n3\n[0, 1, 2, 3, 4, 5, 6]\n4\n[1, 2, 3, 4, 5, 6, 7]\n5\n[2, 3, 4, 5, 6, 7, 8]\n6\n[3, 4, 5, 6, 7, 8, 9]\n7\n[4, 5, 6, 7, 8, 9, 10]\n8\n[5, 6, 7, 8, 9, 10, 11]\n9\n[6, 7, 8, 9, 10, 11, 12]\n10\n[7, 8, 9, 10, 11, 12, 13]\n11\n[8, 9, 10, 11, 12, 13, 12]\n12\n[9, 10, 11, 12, 13, 12, 11]\n13\n[10, 11, 12, 13, 12, 11, 10]\n"
    }
   ],
   "source": [
    "# 4.5 for each utterance and time step, stack 7 MFCC/filterbank features symmetrically distributed around current timestep\n",
    "n = 14\n",
    "for i in range(n):\n",
    "    print(i)\n",
    "    if i < 3:\n",
    "        indices = [j for j in range(3-i,0,-1)]\n",
    "        indices += [y for y in range(7-len(indices))]\n",
    "    elif i > n-4:\n",
    "        indices = [j for j in range(i-3, n)]\n",
    "        indices += [y for y in range(n-2,n-2-(7-len(indices)),-1)]\n",
    "    else:\n",
    "        indices = [x for x in range(i-3,i+4)]\n",
    "    print(indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_window_indices(i, n):\n",
    "    if i < 3:\n",
    "        indices = [j for j in range(3-i,0,-1)]\n",
    "        indices += [y for y in range(7-len(indices))]\n",
    "    elif i > n-4:\n",
    "        indices = [j for j in range(i-3, n)]\n",
    "        indices += [y for y in range(n-2,n-2-(7-len(indices)),-1)]\n",
    "    else:\n",
    "        indices = [x for x in range(i-3,i+4)]\n",
    "    assert len(indices) == 7\n",
    "    return indices\n",
    "\n",
    "\n",
    "def process_data(data, window_features=False):\n",
    "    ''' takes in data loaded from npz file and returns dataframe of features and utteance info '''\n",
    "    filenames = []\n",
    "    lmfccs = []\n",
    "    mspecs = []\n",
    "    targets = []\n",
    "    speakers = []\n",
    "#     utterances = [] # e.g. 'a'\n",
    "#     repetitions = [] # e.g. a, b,... the version of the utterance spoken by the speaker\n",
    "    genders = []\n",
    "\n",
    "    ''' loop over points in train_val set and store data in arrays above'''\n",
    "    for x in data:\n",
    "        filename = x['filename']\n",
    "\n",
    "        # get subject and gender\n",
    "        f = filename.split('/')[-1]\n",
    "        _, gender, speaker, utterance = f.split('\\\\')\n",
    "\n",
    "        genders.append(gender)\n",
    "        speakers.append(speaker)\n",
    "        filenames.append(filename)\n",
    "#         print(x['lmfcc'].shape)\n",
    "        lmfccs.append(x['lmfcc'])\n",
    "        mspecs.append(x['mspec'])\n",
    "        targets.append(x['targets'])\n",
    "    # 4.5 \n",
    "    print('Computing dynamic features')\n",
    "    n = len(lmfccs)\n",
    "    lmfcc_windowed = []\n",
    "    mspec_windowed = []\n",
    "    \n",
    "    # dynamic features for lmfccs\n",
    "    for i in range(n):\n",
    "        lmfcc_len = len(lmfccs[i])\n",
    "        l_utterance = []\n",
    "        for j in range(lmfcc_len):\n",
    "            indices = get_window_indices(j,lmfcc_len) \n",
    "            l_frame = [lmfccs[i][x] for x in indices]\n",
    "            # flatten\n",
    "            l_frame = np.array([y for x in l_frame for y in x])\n",
    "            l_utterance.append(l_frame) \n",
    "        # append to full feature set\n",
    "        lmfcc_windowed.append(l_utterance)\n",
    "    print(lmfcc_windowed[0][0].shape)\n",
    "    #  dynamic features for mspec    \n",
    "    for i in range(n):\n",
    "        mspec_len = len(mspecs[i])\n",
    "        m_utterance = []\n",
    "        for j in range(mspec_len):\n",
    "            indices = get_window_indices(j, mspec_len) \n",
    "            m_frame = np.array([mspecs[i][x] for x in indices])\n",
    "            m_frame = np.array([y for x in m_frame for y in x])\n",
    "            m_utterance.append(m_frame)\n",
    "        # append to full feature set\n",
    "        mspec_windowed.append(m_utterance)\n",
    "\n",
    "\n",
    "    df = pd.DataFrame({'filename': filenames, 'speaker': speakers, 'gender': genders, \\\n",
    "                   'lmfcc_dynamic': lmfcc_windowed, 'mspec_dynamic': mspec_windowed, \\\n",
    "                           'lmfcc': lmfccs, 'mspec': mspecs, 'target': targets})\n",
    "    \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Computing dynamic features\n(91,)\n"
    }
   ],
   "source": [
    "df_train = process_data(train_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(91,)"
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "df_train['lmfcc_dynamic'][1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(13,)"
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "df_train['lmfcc'][1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(280,)"
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "df_train['mspec_dynamic'][1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(40,)"
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "df_train['mspec'][1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(8623, 8)"
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'df_test' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-0531dc392abd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df_test' is not defined"
     ]
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split df_train into train and val set, keeping speakers in different sets and stratifying by gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(array(['ac', 'ae', 'ag', 'ai', 'aj', 'al', 'an', 'aw', 'bd', 'bh', 'bi',\n        'br', 'ca', 'cb', 'cf', 'cg', 'cl', 'cm', 'cr', 'dc', 'dg', 'dl',\n        'dn', 'ea', 'ec', 'ee', 'eg', 'eh', 'ei', 'ek', 'el', 'es', 'fc',\n        'fd', 'ff', 'fi', 'fj', 'fk', 'fl', 'gg', 'gj', 'gr', 'gt', 'ha',\n        'hg', 'hh', 'hl', 'hn', 'hp', 'hs', 'ie', 'if', 'ig', 'ih', 'il',\n        'in', 'it', 'jc', 'ji', 'jj', 'jn', 'jp', 'jr', 'jt', 'kc', 'kd',\n        'kf', 'kh', 'kk', 'kn', 'kp', 'kr', 'kt', 'la', 'ld', 'lh', 'ls',\n        'lt', 'mb', 'mh', 'mk', 'mm', 'mp', 'mr', 'ms', 'mw', 'nc', 'ne',\n        'nf', 'ng', 'nh', 'ni', 'nr', 'nw', 'pb', 'pd', 'pe', 'pi', 'pk',\n        'pm', 'pp', 'ra', 'rd', 're', 'rf', 'rn', 'rp', 'rr', 'rs', 'sj',\n        'sp', 'st'], dtype=object),\n array([77, 77, 76, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77,\n        77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77,\n        77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77,\n        77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77,\n        77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77,\n        77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77,\n        77, 77, 77, 77, 77, 77, 77, 77, 77, 77], dtype=int64))"
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "np.unique(df_train['speaker'], return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that each speaker has roughly an equal number of utterancesin the train_val set, hence we can do a stratify split by gender into the train-val set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_speakers = df_train[['speaker', 'gender']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "number of speakers in train-val set 112\n"
    }
   ],
   "source": [
    "print('number of speakers in train-val set', len(list(train_speakers['speaker'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "train, val = train_test_split(train_speakers, train_size=0.9, random_state=0, stratify=train_speakers['gender'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check what the gender distribution is in the train, val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([ 0.33635156, -0.21908844, -0.36355532, -0.49843517, -1.39492037,\n       -0.18311234, -0.26989952, -0.49467826, -2.19535765, -1.67742775,\n       -0.61662301,  0.15560889,  1.79280077,  1.67083037, -0.26102995,\n       -1.23877981, -0.10652852,  0.97001643, -0.24019016, -0.08991728,\n       -0.65089171, -0.2554614 ,  1.23605617,  1.03063507,  1.36728737,\n        0.82889043,  0.18422378,  0.50751303,  1.73446129,  1.70585345,\n        1.87938942,  1.01981264,  1.2567082 ,  1.30574304,  1.79602232,\n        3.00156044,  2.43952502,  1.94779432,  2.7101488 ,  2.18438797,\n        1.19276804,  1.14961276,  0.74028984, -0.97944721, -1.76410695,\n       -0.25597138,  0.24631659,  0.48100892, -0.40626892, -0.34258786,\n       -1.12367655, -0.72095253, -0.9405887 , -0.91485584, -0.30200621,\n        1.05488609,  0.70698156,  0.24464445, -0.07465874,  0.50228125,\n        0.57457269,  1.36950666,  0.88794672,  1.47302525,  1.56195474,\n        0.68496624,  1.29119423,  1.73719145,  0.78822844,  0.03386062,\n        0.41113099,  1.56724747,  1.44662793,  2.13008938,  1.97382958,\n        2.9304214 ,  2.24404709,  2.45498764,  2.34741593,  2.90633181,\n        1.60530857,  2.19633052,  1.17036166, -1.208674  , -1.57776344,\n       -0.53216461, -0.24947143, -0.36762463, -0.84166864,  0.14955498,\n        0.14167252,  0.56595016,  0.14476855,  0.94147379,  1.16741439,\n       -0.71261345, -1.08465841, -0.71063916,  0.55104478,  0.51229254,\n        0.55764406, -0.78689144,  0.00307711,  1.51011459,  1.51769025,\n        0.37023456,  1.38649984,  0.48377254,  0.58802604,  1.13219244,\n        1.63893686,  1.9198208 ,  2.00956788,  2.25776852,  2.86303922,\n        2.04952712,  1.84371909,  2.60116746,  2.32944475,  2.02390914,\n        0.26574992,  0.80217018, -0.70216558,  0.36323066, -0.02403469,\n        0.30309502,  0.39240083, -0.06091795,  0.05959541, -0.26479307,\n        0.15210151,  0.71634254,  0.23302034, -0.0064267 , -0.25103951,\n       -1.86209993, -0.11935218, -0.57131208, -0.73511474,  0.3560706 ,\n       -0.02152327, -0.72279452, -0.29063308,  1.56079014,  0.80977251,\n       -0.57251568,  0.79838144,  1.6424054 ,  2.13917028,  1.79348772,\n        2.2492145 ,  1.76323191,  1.62516514,  1.25148132,  1.0864565 ,\n        2.12480986,  2.33175758,  2.45097455,  2.16668398,  2.28698944,\n        1.60530857,  2.19633052,  1.17036166, -1.208674  , -1.57776344,\n       -0.53216461, -0.24947143, -0.36762463, -0.84166864,  0.14955498,\n        0.14167252,  0.56595016,  0.14476855,  0.94147379,  1.16741439,\n       -0.71261345, -1.08465841, -0.71063916,  0.55104478,  0.51229254,\n        0.55764406, -0.78689144,  0.00307711,  1.51011459,  1.51769025,\n        0.37023456,  1.38649984,  0.48377254,  0.58802604,  1.13219244,\n        1.63893686,  1.9198208 ,  2.00956788,  2.25776852,  2.86303922,\n        2.04952712,  1.84371909,  2.60116746,  2.32944475,  2.02390914,\n        1.19276804,  1.14961276,  0.74028984, -0.97944721, -1.76410695,\n       -0.25597138,  0.24631659,  0.48100892, -0.40626892, -0.34258786,\n       -1.12367655, -0.72095253, -0.9405887 , -0.91485584, -0.30200621,\n        1.05488609,  0.70698156,  0.24464445, -0.07465874,  0.50228125,\n        0.57457269,  1.36950666,  0.88794672,  1.47302525,  1.56195474,\n        0.68496624,  1.29119423,  1.73719145,  0.78822844,  0.03386062,\n        0.41113099,  1.56724747,  1.44662793,  2.13008938,  1.97382958,\n        2.9304214 ,  2.24404709,  2.45498764,  2.34741593,  2.90633181,\n        0.33635156, -0.21908844, -0.36355532, -0.49843517, -1.39492037,\n       -0.18311234, -0.26989952, -0.49467826, -2.19535765, -1.67742775,\n       -0.61662301,  0.15560889,  1.79280077,  1.67083037, -0.26102995,\n       -1.23877981, -0.10652852,  0.97001643, -0.24019016, -0.08991728,\n       -0.65089171, -0.2554614 ,  1.23605617,  1.03063507,  1.36728737,\n        0.82889043,  0.18422378,  0.50751303,  1.73446129,  1.70585345,\n        1.87938942,  1.01981264,  1.2567082 ,  1.30574304,  1.79602232,\n        3.00156044,  2.43952502,  1.94779432,  2.7101488 ,  2.18438797])"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "df_train['mspec_dynamic'].values[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train set: 49 men vs 51 women\nTrain set: 6 men vs 6 women\n"
    }
   ],
   "source": [
    "print('Train set:', len(train[train['gender']=='man']), 'men vs', len(train[train['gender']=='woman']), 'women')\n",
    "print('Train set:', len(val[val['gender']=='man']), 'men vs', len(val[val['gender']=='woman']), 'women')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retreive back full data\n",
    "train_speakers = list(train['speaker'])\n",
    "df_val = df_train[~df_train['speaker'].isin(train_speakers)]\n",
    "df_train = df_train[df_train['speaker'].isin(train_speakers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(7699, 8)"
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(924, 8)"
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "df_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Index(['filename', 'speaker', 'gender', 'lmfcc_dynamic', 'mspec_dynamic',\n       'lmfcc', 'mspec', 'target'],\n      dtype='object')"
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "df_train.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'data/{dataset}_lmfcc.npy', np.array(df['lmfcc']))\n",
    "np.save(f'data/{dataset}_lmfcc_dynamic.npy', np.array(df['lmfcc_dynamic']))\n",
    "np.save(f'data/{dataset}_mspec.npy', np.array(df['mspec']))\n",
    "np.save(f'data/{dataset}_targets.npy', np.array(df['targets']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-03439c472059>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'data/{dataset}_mspec_dynamic.npy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mspec_dynamic'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "np.save(f'data/{dataset}_mspec_dynamic.npy', np.array(df['mspec_dynamic']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('data/train_mspec_dynamic','wb') as f:\n",
    "    pickle.dump(np.array(df_train['mspec_dynamic']), f)\n",
    "np.save('data/train_mspec_dynamic.npy', np.array(df_train['mspec_dynamic']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data/train_mspec_dynamic.npy', np.array(df_train['mspec_dynamic']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data/train_lmfcc_dynamic.npy', np.array(df_train['lmfcc_dynamic']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data/train_target_dynamic.npy', np.array(df_train['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data/val_mspec_dynamic.npy', np.array(df_val['mspec_dynamic']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data/val_lmfcc_dynamic.npy', np.array(df_val['lmfcc_dynamic']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data/val_target_dynamic.npy', np.array(df_val['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(df, dataset):\n",
    "\n",
    "    np.save(f'data/{dataset}_lmfcc.npy', np.array(df['lmfcc']))\n",
    "    np.save(f'data/{dataset}_lmfcc_dynamic.npy', np.array(df['lmfcc_dynamic']))\n",
    "    np.save(f'data/{dataset}_mspec.npy', np.array(df['mspec']))\n",
    "    np.save(f'data/{dataset}_mspec_dynamic.npy', np.array(df['mspec_dynamic']))\n",
    "    np.save(f'data/{dataset}_targets.npy', np.array(df['targets']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data(df_train, 'data/train')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_data\n",
    "del df_train\n",
    "del df_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=np.load('data/testdata.npz',allow_pickle=True)['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Computing dynamic features\n(91,)\n"
    }
   ],
   "source": [
    "df_test = process_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Index(['filename', 'speaker', 'gender', 'lmfcc_dynamic', 'mspec_dynamic',\n       'lmfcc', 'mspec', 'target'],\n      dtype='object')"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "df_test.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data/test_mspec_dynamic.npy', np.array(df_test['mspec_dynamic']))\n",
    "np.save('data/test_lmfcc_dynamic.npy', np.array(df_test['lmfcc_dynamic']))\n",
    "np.save('data/test_target_dynamic.npy', np.array(df_test['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(8700,)"
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "df_test['mspec_dynamic'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-df2e12d489ec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mspec_dynamic'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "df_test['mspec_dynamic'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'save_data' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-9d38dd136bfa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msave_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'data/test.npz'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'save_data' is not defined"
     ]
    }
   ],
   "source": [
    "save_data(df_test, 'data/test.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_data_npz(fpath, feature, dynamic=False):\n",
    "    data = np.load(fpath, allow_pickle=True)['data'].item()\n",
    "    if dynamic:\n",
    "        print('using dynamic features')\n",
    "        features = data[feature+'_dynamic']\n",
    "    else:\n",
    "        print('using ordinary (not dynamic) features')\n",
    "        features = data[feature]\n",
    "    targets = np.array(data['targets'])\n",
    "    return features, targets\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using dynamic features\n"
     ]
    }
   ],
   "source": [
    "lmfccs, targets = load_data_npz('data/train.npz', feature='lmfcc', dynamic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lmfccs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(198, 13)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lmfccs[0][4].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Normalise features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using ordinary (not dynamic) features\n",
      "using ordinary (not dynamic) features\n",
      "using ordinary (not dynamic) features\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'data'\n",
    "dynamic = False\n",
    "train_lmfccs, train_targets = load_data_npz(data_dir+'/train.npz', feature='lmfcc', dynamic=dynamic)\n",
    "val_lmfccs, val_targets = load_data_npz(data_dir+'/val.npz', feature='lmfcc', dynamic=dynamic)\n",
    "test_lmfccs, test_targets = load_data_npz(data_dir+'/test.npz', feature='lmfcc', dynamic=dynamic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "def flatten(data):\n",
    "    flattened_data = np.array([y for x in data for y in x])\n",
    "    return flattened_data\n",
    "\n",
    "\n",
    "def scale(train_features, val_features, test_features):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train_features)\n",
    "    train_features = scaler.transform(train_features)\n",
    "    val_features = scaler.transform(val_features)\n",
    "    test_features = scaler.transform(test_features)\n",
    "    return train_features, val_features, test_features\n",
    "\n",
    "\n",
    "def normalise_over_train(train_x, val_x, test_x, train_y, val_y, test_y):\n",
    "    ''' normalise over training set and apply parameters to val, test set'''\n",
    "    # flatten\n",
    "    train_x = flatten(train_x)\n",
    "    val_x = flatten(val_x)\n",
    "    test_x = flatten(test_x)\n",
    "    train_y = flatten(train_y)\n",
    "    val_y = flatten(val_y)\n",
    "    test_y = flatten(test_y)\n",
    "\n",
    "    # normalise\n",
    "    train_x, val_x, test_x = scale(train_x, val_x, test_x)\n",
    "\n",
    "    return train_x, val_x, test_x, train_y, val_y, test_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, x_test, y_train, y_val, y_test = normalise_over_train(train_lmfccs, val_lmfccs, test_lmfccs, \\\n",
    "                                                                      train_targets, val_targets, test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n classes 61\n"
     ]
    }
   ],
   "source": [
    "# one hot encode targets\n",
    "from keras.utils import np_utils\n",
    "n_classes = len(np.load('data/StateList.npy'))\n",
    "print('n classes', n_classes)\n",
    "y_train = np_utils.to_categorical(np.array(y_train), n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1342672, 13)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1342672,)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(164720, 13)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(164720,)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1527014, 13)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1527014,)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python37664bitb225f4bd86994289a4055ad484dc0491"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}